{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Egeria Logo](https://raw.githubusercontent.com/odpi/egeria/master/assets/img/ODPi_Egeria_Logo_color.png)\n",
    "\n",
    "### Egeria Hands-On Lab\n",
    "# Welcome to the Performance Test Suite Results Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Egeria is an open source project that provides open standards and implementation libraries to connect tools, catalogs and platforms together so they can share information (called metadata) about data and the technology that supports it.\n",
    "\n",
    "In this hands-on lab you will get a chance to analyze the results of running a performance test suite against a repository. This assumes you have already followed the [run-performance-test-suite.ipynb](run-performance-test-suite.ipynb) lab, or another method, to produce such results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Performance Suite \n",
    "\n",
    "The Performance Suite can be used to test a Repository Connector to record the performance of its various repository methods.\n",
    "\n",
    "This lab more deeply analyzes the results produced from running the Repository Performance Workbench."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Results\n",
    "\n",
    "Before running this lab, you will need to have some results to analyze. As mentioned in the introduction, this could be by running through the Performance Test Suite Lab or through other means.\n",
    "\n",
    "Irrespective of how you have run the suite, you will need to have the following files available:\n",
    "\n",
    "- `profile-details/*.json` (there should be one such JSON file for each profile of the Performance Suite).\n",
    "\n",
    "You may have other files as well (such as `test-case-details/*.json`), but at the moment these are not used by this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the location of the results\n",
    "\n",
    "This lab does not actually require a running Egeria instance, since we will only be analyzing the results that have already been produced.\n",
    "\n",
    "Instead, we need to start by specifying a preferred name for how we will refer to the results, and the actual location of those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_results_name       = \"Crux\"\n",
    "primary_results_location   = \"crux-5-2-rocks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method will simply verify that the defined locations exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def validateProfileResultsLocation(location):\n",
    "    profile_details_location = location + os.path.sep + \"profile-details\"\n",
    "    print(\"Validating profile-details location:\", profile_details_location)\n",
    "    if os.path.isdir(profile_details_location):\n",
    "        print(\" ... directory exists.\")\n",
    "    else:\n",
    "        print(\" ... ERROR: could not find this directory. Is the location specified correct?\")\n",
    "\n",
    "validateProfileResultsLocation(primary_results_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing details\n",
    "\n",
    "Now that we have the location of the results, we will parse the JSON files for each profile into Python data structures (specifically, a Pandas dataframe) that we can then use for deeper analysis and visualization.\n",
    "\n",
    "To do this, we will define a number of re-usable functions as follows:\n",
    "\n",
    "### `parseEvidence`\n",
    "\n",
    "Parses all of the `positiveTestEvidence` objects from a given set of requirement results in a profile, in order to capture the detailed information about the specific method that was invoked, its execution time, which test case it was running, its assertion, etc.\n",
    "\n",
    "### `parseRequirementResults`\n",
    "\n",
    "Parses all of the requirement results objects from a given profile, in order to hand these off to `parseEvidence` to process.\n",
    "\n",
    "### `getAllProfiles`\n",
    "\n",
    "Retrieves all of the profile JSON files from the `profile-details` directory of the specified results location.\n",
    "\n",
    "### `parseProfileDetailsIntoDF`\n",
    "\n",
    "Parses the details of a given profile result JSON file into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the profile ordering\n",
    "profile_order=[\n",
    "    'Entity creation', 'Entity search', 'Relationship creation', 'Relationship search',\n",
    "    'Entity classification', 'Classification search', 'Entity update', 'Relationship update',\n",
    "    'Classification update', 'Entity undo', 'Relationship undo', 'Entity retrieval', 'Entity history retrieval',\n",
    "    'Relationship retrieval', 'Relationship history retrieval', 'Entity history search', 'Relationship history search',\n",
    "    'Graph queries', 'Graph history queries', 'Entity re-home', 'Relationship re-home', 'Entity declassify',\n",
    "    'Entity re-type', 'Relationship re-type', 'Entity re-identify', 'Relationship re-identify',\n",
    "    'Relationship delete', 'Entity delete', 'Entity restore', 'Relationship restore', 'Relationship purge',\n",
    "    'Entity purge'\n",
    "]\n",
    "\n",
    "# Given a profileResult.requirementResults object, parse all of its positiveTestEvidence\n",
    "# and group the results by methodName\n",
    "def parseEvidence(df, repositoryName, requirementResults):\n",
    "    if (requirementResults is not None and 'positiveTestEvidence' in requirementResults):\n",
    "        print(\"Parsing evidence for:\", requirementResults['name'], \"(\" + repositoryName + \")\")\n",
    "        data_array = []\n",
    "        for evidence in requirementResults['positiveTestEvidence']:\n",
    "            if ('methodName' in evidence and 'elapsedTime' in evidence):\n",
    "                data = {\n",
    "                    'repo': repositoryName,\n",
    "                    'method_name': evidence['methodName'],\n",
    "                    'elapsed_time': evidence['elapsedTime'],\n",
    "                    'profile_name': requirementResults['name'],\n",
    "                    'test_case_id': evidence['testCaseId'],\n",
    "                    'assertion_id': evidence['assertionId']\n",
    "                }\n",
    "                data_array.append(data)\n",
    "        df = df.append(pd.read_json(json.dumps(data_array), orient='records'), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Given a profile detail JSON file, retrieve all of its profileResult.requirementResults[] objects\n",
    "def parseRequirementResults(profileFile):\n",
    "    with open(profileFile) as f:\n",
    "        profile = json.load(f)\n",
    "    # This first case covers files retrieved via API\n",
    "    if ('profileResult' in profile and 'requirementResults' in profile['profileResult']):\n",
    "        return profile['profileResult']['requirementResults']\n",
    "    # This second case covers files created by the CLI client\n",
    "    elif ('requirementResults' in profile):\n",
    "        return profile['requirementResults']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Retrieve a listing of all of the profile detail JSON files\n",
    "def getAllProfiles(profileLocation):\n",
    "    detailsLocation = profileLocation + os.path.sep + \"profile-details\"\n",
    "    _, _, filenames = next(os.walk(detailsLocation))\n",
    "    full_filenames = []\n",
    "    for filename in filenames:\n",
    "        full_filenames.append(detailsLocation + os.path.sep + filename)\n",
    "    return full_filenames\n",
    "\n",
    "# Parse all of the provided profile file's details into the provided dataframe\n",
    "def parseProfileDetailsIntoDF(df, profileFile, qualifier):\n",
    "    profileResults = parseRequirementResults(profileFile)\n",
    "    if profileResults is not None:\n",
    "        for result in profileResults:\n",
    "            df = parseEvidence(df, qualifier, result)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will simply retrieve all of the profile detail files, and for each one we will parse its details and append them into the DataFrame we will use for later analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_files = getAllProfiles(primary_results_location)\n",
    "\n",
    "df1 = pd.DataFrame({'repo': [], 'method_name': [], 'elapsed_time': [], 'profile_name': [], 'test_case_id': [], 'assertion_id': []})\n",
    "\n",
    "for profile_file in primary_files:\n",
    "    df1 = parseProfileDetailsIntoDF(df1, profile_file, primary_results_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the result details\n",
    "\n",
    "Now that we have a Python-native format for the detailed information, we can begin visualizing the results.\n",
    "\n",
    "We will first define some methods to carry out this deeper analysis. These work as follows:\n",
    "\n",
    "### `plotProfile`\n",
    "\n",
    "Provided a dataframe and profile name, renders a histogram showing the distribution of elapsed times for each method within the specified performance test profile. The histogram shows the number of times that method was called on the Y axis resulting in a specific elapsed response time (in milliseconds) on the X axis. (Overlaid on the histogram is a kernel density estimation showing an approximate continuous distribution of the elapsed time of the responses.)\n",
    "\n",
    "### `slowestRunning`\n",
    "\n",
    "Provided a dataframe and profile name, shows the top-10 slowest-running methods. You can also optionally provide the number to display (rather than the default of 10) and a specific method name to further filter the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "def plotProfile(df, profileName, remove_outliers=False):\n",
    "    dfX = df[df['profile_name'] == profileName]\n",
    "    # Only attempt to plot if there is anything left in the dataframe\n",
    "    if not dfX.empty:\n",
    "        if remove_outliers:\n",
    "            # If we have been asked to remove outliers, drop anything outside the 2nd and 98th percentiles\n",
    "            dfX = dfX[dfX['elapsed_time'].between(dfX['elapsed_time'].quantile(.02), dfX['elapsed_time'].quantile(.98))]\n",
    "        sns.set(font_scale=1.2)\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        fix, axs = plt.subplots(ncols=1, nrows=1, figsize=(18,9))\n",
    "        # Display the methods within the profile in alphabetical order for consistency\n",
    "        methods = dfX['method_name'].unique()\n",
    "        figure = sns.histplot(ax=axs, data=dfX, x=\"elapsed_time\", hue=\"method_name\",\n",
    "                              hue_order=sorted(methods), kde=True, discrete=True)\n",
    "        figure.set(xlabel=\"Elapsed time (ms)\")\n",
    "        figure.set_title(profileName)\n",
    "        figure.get_legend().set(title='Method')\n",
    "        display(fix)\n",
    "        plt.close(fix)\n",
    "\n",
    "def slowestRunning(df, profileName, num=10, methodName=None):\n",
    "    dfX = df[df['profile_name'] == profileName]\n",
    "    if methodName:\n",
    "        dfX = dfX[dfX['method_name'] == methodName]\n",
    "    if not dfX.empty:\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        display(dfX.sort_values(by=['elapsed_time'], ascending=False).head(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our methods defined, we can now visualize the results simply by calling them with appropriate parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing each profile's response times\n",
    "\n",
    "The following plots the response times of each method within each profile, in the order that the profiles themselves were executed within the performance suite. (As this is rendering 30+ detailed visualizations it may take a little time to complete.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in profile_order:\n",
    "    plotProfile(df1, profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these visualizations, we can quickly see the \"typical\" response times for a given method (where the peak(s) are highest) as well as the potential for extreme values (where there is a long-tail to the right that could extend up to several times longer than the \"typical\" response time).\n",
    "\n",
    "For cases where we have such extreme values, we may want to better understand what specific set of parameters has caused that extreme response time: we can do this using the `slowestRunning` method. For example, the following will show us the 10-slowest-running search queries against entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slowestRunning(df1, 'Entity search', num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first entry in the table will be the slowest method in that entire profile: we can check its elapsed time under the `elapsed_time` column, the specific method that was called under the `method_name` column, and then determine more detail about that specific method invocation from the `test_case_id` column (which gives us information on the specific type definition that this method was called against) and the `assertion_id` column (which is unique to a particular point in the performance suite's testing code).\n",
    "\n",
    "For example, if we see that:\n",
    "\n",
    "- `method_name` = `findEntities`\n",
    "- `elapsed_time` = `800`\n",
    "- `test_case_id` = `repository-entity-search-performance-Annotation`\n",
    "- `assertion_id` = `repository-entity-search-performance-findEntities-all-p31`\n",
    "\n",
    "Then we can deduce that our slowest-running search was a `findEntities` method call that retrieved page 31 of the results for all `Annotation` entities, and that this took 800 milliseconds to complete.\n",
    "\n",
    "Reviewing the other entries in the table is useful to determine whether this might be an outlier caused by something else occuring on the system (for example a garbage collection pause), or if there are other similar cases of extreme response times. (If this is the only number that seems to be at this extreme, you may want to re-run the performance suite with the same parameters to determine if you can produce the same result for this same type: if not, it is likely caused by some other area like garbage collection.)\n",
    "\n",
    "If this does not appear to be a one-off outlier, then you will likely want to take this particular combination of parameters and do some more in-depth profiling and analysis of your repository to see what might be causing this extreme response time.\n",
    "\n",
    "On the other hand, if you determine that actually the behavior is \"random\" rather than something inherent in the particular combination of parameters, you may instead want to simply remove such outliers and re-draw the plot for that profile without it, so that you have greater resolution on the more typical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotProfile(df1, 'Entity search', remove_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the outliers, we can more clearly see the typical distribution of each method's response times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results\n",
    "\n",
    "Up to this point, we have done some analysis of the performance of a single repository on its own. However, we may also be interested in comparing and contrasting these results with either different performance test runs against that same repository (for example, after using different volume-controlling parameter settings, to investigate scalability), or by comparing performance test runs against other repositories.\n",
    "\n",
    "In either case, we simply need to load those additional results and compare / contrast accordingly.\n",
    "\n",
    "For example, start by defining a `secondary_results_location` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_results_name     = \"In-memory\"\n",
    "secondary_results_location = \"inmem-5-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now re-use the methods defined earlier to validate, load and delve deeper into these results (note that we are re-using the same dataframe, which will now contain the combined results from both tests):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validateProfileResultsLocation(secondary_results_location)\n",
    "secondary_files = getAllProfiles(secondary_results_location)\n",
    "for profile_file in secondary_files:\n",
    "    df1 = parseProfileDetailsIntoDF(df1, profile_file, secondary_results_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing side-by-side comparisons\n",
    "\n",
    "Now that we have multiple results, we may want to use some additional visualizations to see the relative performance side-by-side. Following are some methods to do so:\n",
    "\n",
    "### `compareProfiles`\n",
    "\n",
    "Will plot the results from two different tests side-by-side for a given profile using a split violin plot. The violin plot shows a kernel density estimation distribution of the response time values for a given method, and is split between the two tests: one test's distribution showing on the left side and the other's on the right.\n",
    "\n",
    "In simple terms: the higher a given plot goes, the slower the response times for that particular method; and the wider it is indicates the more frequently that particular reponse time was observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareProfiles(df, profileName, left, right, remove_outliers=False):\n",
    "    dfX = df[df['profile_name'] == profileName]\n",
    "    # Only attempt to plot if there is anything left in the dataframe\n",
    "    if not dfX.empty:\n",
    "        if remove_outliers:\n",
    "            # If we have been asked to remove outliers, drop anything outside the 2nd and 98th percentiles\n",
    "            dfX = dfX[dfX['elapsed_time'].between(dfX['elapsed_time'].quantile(.02), dfX['elapsed_time'].quantile(.98))]\n",
    "        sns.set(font_scale=1.2)\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        fix, axs = plt.subplots(ncols=1, nrows=1, figsize=(18,9))\n",
    "        # Display the methods within the profile in alphabetical order for consistency\n",
    "        methods = dfX['method_name'].unique()\n",
    "        figure = sns.violinplot(x=\"method_name\", y=\"elapsed_time\", ax=axs, hue=\"repo\",\n",
    "                                hue_order=[left, right], split=True, scale='count',\n",
    "                                inner='quartile', cut=0, data=dfX)\n",
    "        # If there are more than 4 methods in the profile, rotate them so they are still readable\n",
    "        if (len(methods) > 4):\n",
    "            figure.set_xticklabels(figure.get_xticklabels(), rotation=10)\n",
    "        figure.set(xlabel=\"Method name\", ylabel=\"Elapsed time (ms)\")\n",
    "        figure.set_title(profileName + ' comparison')\n",
    "        figure.get_legend().set(title='Test')\n",
    "        display(fix)\n",
    "        plt.close(fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example, if we compare the two results' on their performance of the \"Relationship retrieval\" profile, we can quickly see whether the two are approximately the same speed or if one is generally faster than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareProfiles(df1, 'Relationship retrieval', primary_results_name, secondary_results_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
